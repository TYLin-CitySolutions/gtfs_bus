# name: Build & Publish GTFS Parquet

# on:
#   workflow_dispatch: {}
#   push:
#     branches: [ main ]

# concurrency:
#   group: "pages"
#   cancel-in-progress: true

# jobs:
#   build:
#     runs-on: ubuntu-latest
#     permissions:
#       contents: read
#       pages: write
#       id-token: write
#     steps:
#       - uses: actions/checkout@v4
#       - uses: actions/setup-python@v5
#         with: { python-version: "3.11" }
#       - run: python -m pip install -r requirements.txt

#       # Build per-feed Parquet into parquet/<table>/<feed>.parquet
#       - name: Run ingest
#         run: python ingest/ingest_gtfs.py

#       # IMPORTANT GITHUB PAGES GOTCHA:
#       # HTTP servers (like Pages) don't support wildcards for listing folders.
#       # Create ONE combined file per table so your app can read a single URL.
#       - name: Combine Parquet into single files
#         run: |
#           python - <<'PY'
#           import duckdb, os
#           os.makedirs('parquet_combined', exist_ok=True)
#           con = duckdb.connect()
#           for tbl in ['dim_stops','dim_trips','dim_routes','calendar_base','fact_stop_events']:
#               src = f"parquet/{tbl}/*.parquet"
#               dst = f"parquet_combined/{tbl}.parquet"
#               con.execute(f"COPY (SELECT * FROM read_parquet('{src}')) TO '{dst}' (FORMAT PARQUET, COMPRESSION 'ZSTD');")
#           print("Wrote combined files to parquet_combined/")
#           PY

#       # Publish combined files to GitHub Pages
#       - name: Upload artifact
#         uses: actions/upload-pages-artifact@v3
#         with: { path: 'parquet_combined' }

#       - name: Deploy to GitHub Pages
#         uses: actions/deploy-pages@v4
